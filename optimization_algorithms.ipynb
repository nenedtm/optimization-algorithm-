{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Gradient-Based Optimization Algorithms\n",
        "\n",
        "This project implements and compares several gradient-based optimization algorithms\n",
        "commonly used in machine learning and deep learning:\n",
        "\n",
        "* **Gradient Descent (GD)**:\n",
        "  \n",
        "  The basic optimization algorithm that updates parameters in the direction of the\n",
        "  negative gradient of the loss function. Uses the entire dataset for each update.\n",
        "  \n",
        "  - Simple and guaranteed to converge for convex functions with appropriate learning rate\n",
        "  - Computationally expensive for large datasets\n",
        "  - Can be slow to converge\n",
        "\n",
        "* **Stochastic Gradient Descent (SGD)**:\n",
        "  \n",
        "  Updates parameters using only one randomly selected sample at a time.\n",
        "  \n",
        "  - Much faster updates than GD\n",
        "  - High variance in updates can lead to noisy convergence\n",
        "  - Can escape local minima due to noise\n",
        "  - Requires careful learning rate tuning\n",
        "\n",
        "* **Mini-Batch Gradient Descent (Batch GD)**:\n",
        "  \n",
        "  Compromise between GD and SGD - uses small batches of samples for each update.\n",
        "  \n",
        "  - Reduces variance compared to SGD\n",
        "  - More efficient than full GD\n",
        "  - Can leverage vectorization for speed\n",
        "  - Most commonly used in practice\n",
        "\n",
        "* **Adam (Adaptive Moment Estimation)**:\n",
        "  \n",
        "  Combines ideas from RMSprop and momentum. Maintains adaptive learning rates\n",
        "  for each parameter and uses both first and second moments of gradients.\n",
        "  \n",
        "  - Adaptive learning rates per parameter\n",
        "  - Works well with sparse gradients\n",
        "  - Requires little tuning\n",
        "  - Most popular optimizer in deep learning\n",
        "\n",
        "* **Nesterov Accelerated Gradient (NAG)**:\n",
        "  \n",
        "  Improvement over momentum-based methods. Looks ahead by calculating gradient\n",
        "  at the anticipated future position.\n",
        "  \n",
        "  - Better convergence properties than standard momentum\n",
        "  - Particularly effective for convex functions\n",
        "  - Reduces oscillations\n",
        "\n",
        "* **Adan (Adaptive Nesterov Momentum)**:\n",
        "  \n",
        "  Recent optimizer combining adaptive learning rates with Nesterov momentum.\n",
        "  \n",
        "  - Faster convergence than Adam in many cases\n",
        "  - More stable training\n",
        "  - Effective for both convex and non-convex optimization\n"
      ],
      "metadata": {
        "id": "vO7hCXJN1ukp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Co1zWxa51loL"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test functions for optimization\n",
        "def rosenbrock(x, y):\n",
        "    #Rosenbrock function: a classic non-convex optimization test function\n",
        "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
        "\n",
        "def rosenbrock_grad(x, y):\n",
        "    #Gradient of the Rosenbrock function\n",
        "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
        "    dy = 200 * (y - x**2)\n",
        "    return np.array([dx, dy])\n",
        "\n",
        "def beale(x, y):\n",
        "    #Beale function: another test function with multiple local minima\n",
        "    term1 = (1.5 - x + x*y)**2\n",
        "    term2 = (2.25 - x + x*y**2)**2\n",
        "    term3 = (2.625 - x + x*y**3)**2\n",
        "    return term1 + term2 + term3\n",
        "\n",
        "def beale_grad(x, y):\n",
        "    #Gradient of the Beale function\n",
        "    dx = (2 * (1.5 - x + x*y) * (y - 1) +\n",
        "          2 * (2.25 - x + x*y**2) * (y**2 - 1) +\n",
        "          2 * (2.625 - x + x*y**3) * (y**3 - 1))\n",
        "    dy = (2 * (1.5 - x + x*y) * x +\n",
        "          2 * (2.25 - x + x*y**2) * 2*x*y +\n",
        "          2 * (2.625 - x + x*y**3) * 3*x*y**2)\n",
        "    return np.array([dx, dy])\n",
        "\n",
        "def sphere(x, y):\n",
        "    #Sphere function: simple convex quadratic function\n",
        "    return x**2 + y**2\n",
        "\n",
        "def sphere_grad(x, y):\n",
        "    #Gradient of the Sphere function\n",
        "    return np.array([2*x, 2*y])\n",
        "\n",
        "# Numerical gradient computation for custom functions\n",
        "def numerical_gradient(func, x, y, h=1e-5):\n",
        "    grad_x = (func(x + h, y) - func(x - h, y)) / (2 * h)\n",
        "    grad_y = (func(x, y + h) - func(x, y - h)) / (2 * h)\n",
        "    return np.array([grad_x, grad_y])\n",
        "\n",
        "# Function to get custom function from user\n",
        "def get_custom_function():\n",
        "    print(\"\\nEnter your custom function f(x, y):\")\n",
        "    print(\"\\nUse np notation\")\n",
        "    func_str = input(\"\\nYour function f(x,y) = \")\n",
        "\n",
        "    def custom_func(x, y):\n",
        "        try:\n",
        "            return eval(func_str)\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating function: {e}\")\n",
        "            return 0\n",
        "    try:\n",
        "        test_val = custom_func(1.0, 1.0)\n",
        "        print(f\"Test: f(1, 1) = {test_val}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing function: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    def custom_grad(x, y):\n",
        "        return numerical_gradient(custom_func, x, y)\n",
        "\n",
        "    return custom_func, custom_grad\n",
        "\n",
        "# Get user choice\n",
        "print(\"Available test functions:\")\n",
        "print(\"1. Rosenbrock function (challenging non-convex)\")\n",
        "print(\"2. Beale function (multiple local minima)\")\n",
        "print(\"3. Sphere function (simple convex)\")\n",
        "print(\"4. Custom function (enter your own)\")\n",
        "\n",
        "function_choice = int(input(\"\\nSelect function (1-4): \"))\n",
        "\n",
        "if function_choice == 1:\n",
        "    objective_func = rosenbrock\n",
        "    gradient_func = rosenbrock_grad\n",
        "    x_range = (-2, 2)\n",
        "    y_range = (-1, 3)\n",
        "    start_point = np.array([-1.5, 2.5])\n",
        "    func_name = \"Rosenbrock\"\n",
        "elif function_choice == 2:\n",
        "    objective_func = beale\n",
        "    gradient_func = beale_grad\n",
        "    x_range = (-4.5, 4.5)\n",
        "    y_range = (-4.5, 4.5)\n",
        "    start_point = np.array([0.5, 0.5])\n",
        "    func_name = \"Beale\"\n",
        "elif function_choice == 3:\n",
        "    objective_func = sphere\n",
        "    gradient_func = sphere_grad\n",
        "    x_range = (-10, 10)\n",
        "    y_range = (-10, 10)\n",
        "    start_point = np.array([8.0, 8.0])\n",
        "    func_name = \"Sphere\"\n",
        "else:  # Custom function\n",
        "    objective_func, gradient_func = get_custom_function()\n",
        "\n",
        "    if objective_func is None:\n",
        "        print(\"Error with custom function. Using Sphere function instead.\")\n",
        "        objective_func = sphere\n",
        "        gradient_func = sphere_grad\n",
        "        x_range = (-10, 10)\n",
        "        y_range = (-10, 10)\n",
        "        start_point = np.array([8.0, 8.0])\n",
        "        func_name = \"Sphere\"\n",
        "    else:\n",
        "        print(\"\\nSet the visualization range:\")\n",
        "        x_min = float(input(\"x min: \"))\n",
        "        x_max = float(input(\"x max: \"))\n",
        "        y_min = float(input(\"y min: \"))\n",
        "        y_max = float(input(\"y max: \"))\n",
        "        x_range = (x_min, x_max)\n",
        "        y_range = (y_min, y_max)\n",
        "\n",
        "        print(\"\\nSet the starting point for optimization:\")\n",
        "        start_x = float(input(\"Starting x: \"))\n",
        "        start_y = float(input(\"Starting y: \"))\n",
        "        start_point = np.array([start_x, start_y])\n",
        "        func_name = \"Custom\"\n",
        "\n",
        "# Get optimization parameters with function-specific suggestions\n",
        "if func_name == \"Rosenbrock\":\n",
        "    suggested_lr = \"0.0001-0.001\"\n",
        "    default_lr = 0.001\n",
        "elif func_name == \"Beale\":\n",
        "    suggested_lr = \"0.001-0.005\"\n",
        "    default_lr = 0.002\n",
        "elif func_name == \"Sphere\":\n",
        "    suggested_lr = \"0.01-0.1\"\n",
        "    default_lr = 0.01\n",
        "else:  # Custom\n",
        "    suggested_lr = \"0.001-0.01\"\n",
        "    default_lr = 0.001\n",
        "\n",
        "print(f\"\\nRecommended learning rate for {func_name}: {suggested_lr}\")\n",
        "learning_rate = float(input(f\"Enter learning rate (press Enter for {default_lr}): \") or default_lr)\n",
        "num_iterations = int(input(\"Enter number of iterations (suggested: 100-1000, press Enter for 500): \") or 500)\n",
        "batch_size = int(input(\"Enter batch size for mini-batch GD (suggested: 10-50, press Enter for 32): \") or 32)"
      ],
      "metadata": {
        "id": "SKeOQDbP3fDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective function plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "x = np.linspace(x_range[0], x_range[1], 400)\n",
        "y = np.linspace(y_range[0], y_range[1], 400)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = objective_func(X, Y)\n",
        "\n",
        "contour = plt.contour(X, Y, Z, levels=40, cmap='viridis')\n",
        "plt.clabel(contour, inline=True, fontsize=8)\n",
        "\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(f\"Objective Function: {func_name}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SvgLv_LI_mWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Foundations\n",
        "\n",
        "### General Optimization Problem\n",
        "\n",
        "We aim to minimize a function $f(\\theta)$ where $\\theta$ represents the parameters.\n",
        "\n",
        "$$\n",
        "\\min_{\\theta} f(\\theta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Gradient Descent Update Rule\n",
        "\n",
        "The basic Gradient Descent update is:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, \\nabla f(\\theta_t)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\theta_t$ is the parameter vector at iteration $t$\n",
        "- $\\eta$ is the learning rate (step size)\n",
        "- $\\nabla f(\\theta_t)$ is the gradient of $f$ evaluated at $\\theta_t$\n",
        "\n",
        "---\n",
        "\n",
        "### Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Instead of computing the gradient over the entire dataset, SGD uses a single sample:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, \\nabla f_i(\\theta_t)\n",
        "$$\n",
        "\n",
        "where $f_i$ is the loss for the $i$-th sample.\n",
        "\n",
        "---\n",
        "\n",
        "### Mini-Batch Gradient Descent\n",
        "\n",
        "Uses a subset (mini-batch) $B$ of the data:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, \\frac{1}{|B|} \\sum_{i \\in B} \\nabla f_i(\\theta_t)\n",
        "$$\n",
        "\n",
        "where $|B|$ is the size of the mini-batch.\n",
        "\n",
        "---\n",
        "\n",
        "### Momentum\n",
        "\n",
        "Adds a velocity term to accelerate convergence:\n",
        "\n",
        "$$\n",
        "v_{t+1} = \\beta v_t + \\nabla f(\\theta_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, v_{t+1}\n",
        "$$\n",
        "\n",
        "where $\\beta$ is the momentum coefficient (typically $\\beta = 0.9$).\n",
        "\n",
        "---\n",
        "\n",
        "### Nesterov Accelerated Gradient\n",
        "\n",
        "Looks ahead before computing the gradient:\n",
        "\n",
        "$$\n",
        "v_{t+1} = \\beta v_t + \\nabla f(\\theta_t - \\eta \\beta v_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, v_{t+1}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Maintains running averages of both the gradient and its square:\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla f(\\theta_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "v_t = \\beta_2 v_{t-1} + (1-\\beta_2)\\left[\\nabla f(\\theta_t)\\right]^2\n",
        "$$\n",
        "\n",
        "Bias correction:\n",
        "\n",
        "$$\n",
        "\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n",
        "$$\n",
        "\n",
        "Update rule:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "$$\n",
        "\n",
        "Typical values: $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=10^{-8}$.\n",
        "\n",
        "---\n",
        "\n",
        "### Adan\n",
        "\n",
        "Combines adaptive learning with Nesterov momentum and uses gradient differences:\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla f(\\theta_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "v_t = \\beta_2 v_{t-1} + (1-\\beta_2)\\left[\\nabla f(\\theta_t) - \\nabla f(\\theta_{t-1})\\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "n_t = \\beta_3 n_{t-1} + (1-\\beta_3)\\left[\\nabla f(\\theta_t) + \\beta_2(\\nabla f(\\theta_t) - \\nabla f(\\theta_{t-1}))\\right]^2\n",
        "$$\n",
        "\n",
        "Update rule:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\, \\frac{m_t + \\beta_2 v_t}{\\sqrt{n_t} + \\epsilon}\n",
        "$$"
      ],
      "metadata": {
        "id": "RHjrcPy-2EKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Optimization Algorithm Implementations\n",
        "def gradient_descent(grad_func, start, lr, iterations, clip_value=10.0):\n",
        "    #Standard Gradient Descent. Uses the full gradient at each step\n",
        "    path = [start.copy()]\n",
        "    theta = start.copy()\n",
        "\n",
        "    for i in range(iterations):\n",
        "        grad = grad_func(theta[0], theta[1])\n",
        "        grad_norm = np.linalg.norm(grad)\n",
        "        if grad_norm > clip_value:\n",
        "            grad = grad * (clip_value / grad_norm)\n",
        "\n",
        "        theta = theta - lr * grad\n",
        "\n",
        "        if np.any(np.isnan(theta)) or np.any(np.isinf(theta)):\n",
        "            print(f\"Warning: GD diverged at iteration {i}. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        path.append(theta.copy())\n",
        "\n",
        "    return np.array(path)\n",
        "\n",
        "def stochastic_gradient_descent(grad_func, start, lr, iterations, clip_value=10.0):\n",
        "    # Stochastic Gradient Descent. Adds noise to simulate using single samples\n",
        "    path = [start.copy()]\n",
        "    theta = start.copy()\n",
        "\n",
        "    for i in range(iterations):\n",
        "        grad = grad_func(theta[0], theta[1])\n",
        "        grad_norm = np.linalg.norm(grad)\n",
        "        if grad_norm > clip_value:\n",
        "            grad = grad * (clip_value / grad_norm)\n",
        "\n",
        "        noise = np.random.randn(2) * 0.1\n",
        "        noisy_grad = grad + noise\n",
        "        theta = theta - lr * noisy_grad\n",
        "\n",
        "        if np.any(np.isnan(theta)) or np.any(np.isinf(theta)):\n",
        "            print(f\"Warning: SGD diverged at iteration {i}. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        path.append(theta.copy())\n",
        "\n",
        "    return np.array(path)\n",
        "\n",
        "def batch_gradient_descent(grad_func, start, lr, iterations, batch_size, clip_value=10.0):\n",
        "    # Mini-Batch Gradient Descent. Averages gradients over small batches\n",
        "    path = [start.copy()]\n",
        "    theta = start.copy()\n",
        "\n",
        "    for i in range(iterations):\n",
        "        batch_grad = np.zeros(2)\n",
        "        for _ in range(batch_size):\n",
        "            grad = grad_func(theta[0], theta[1])\n",
        "            grad_norm = np.linalg.norm(grad)\n",
        "            if grad_norm > clip_value:\n",
        "                grad = grad * (clip_value / grad_norm)\n",
        "\n",
        "            noise = np.random.randn(2) * 0.05\n",
        "            batch_grad += (grad + noise)\n",
        "        batch_grad /= batch_size\n",
        "\n",
        "        theta = theta - lr * batch_grad\n",
        "\n",
        "        if np.any(np.isnan(theta)) or np.any(np.isinf(theta)):\n",
        "            print(f\"Warning: Mini-Batch GD diverged at iteration {i}. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        path.append(theta.copy())\n",
        "\n",
        "    return np.array(path)\n",
        "\n",
        "def nesterov_gradient_descent(grad_func, start, lr, iterations, momentum=0.9, clip_value=10.0):\n",
        "    # Nesterov Accelerated Gradient. Looks ahead before computing gradient\n",
        "    path = [start.copy()]\n",
        "    theta = start.copy()\n",
        "    velocity = np.zeros(2)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Look-ahead position\n",
        "        theta_ahead = theta - momentum * velocity\n",
        "        grad = grad_func(theta_ahead[0], theta_ahead[1])\n",
        "\n",
        "        grad_norm = np.linalg.norm(grad)\n",
        "        if grad_norm > clip_value:\n",
        "            grad = grad * (clip_value / grad_norm)\n",
        "\n",
        "        velocity = momentum * velocity + grad\n",
        "        theta = theta - lr * velocity\n",
        "\n",
        "        if np.any(np.isnan(theta)) or np.any(np.isinf(theta)):\n",
        "            print(f\"Warning: Nesterov diverged at iteration {i}. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        path.append(theta.copy())\n",
        "\n",
        "    return np.array(path)\n",
        "\n",
        "def adam_optimizer(grad_func, start, lr, iterations, beta1=0.9, beta2=0.999, epsilon=1e-8, clip_value=10.0):\n",
        "    #Adam Optimizer. Adaptive learning rate with momentum\n",
        "\n",
        "    path = [start.copy()]\n",
        "    theta = start.copy()\n",
        "    m = np.zeros(2)  # First moment\n",
        "    v = np.zeros(2)  # Second moment\n",
        "\n",
        "    for t in range(1, iterations + 1):\n",
        "        grad = grad_func(theta[0], theta[1])\n",
        "\n",
        "        grad_norm = np.linalg.norm(grad)\n",
        "        if grad_norm > clip_value:\n",
        "            grad = grad * (clip_value / grad_norm)\n",
        "\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
        "\n",
        "        m_hat = m / (1 - beta1 ** t)\n",
        "        v_hat = v / (1 - beta2 ** t)\n",
        "\n",
        "        theta = theta - lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "\n",
        "        if np.any(np.isnan(theta)) or np.any(np.isinf(theta)):\n",
        "            print(f\"Warning: Adam diverged at iteration {t}. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        path.append(theta.copy())\n",
        "\n",
        "    return np.array(path)\n",
        "\n",
        "def adan_optimizer(grad_func, start, lr, iterations, beta1=0.98, beta2=0.92, beta3=0.99, epsilon=1e-8, clip_value=10.0):\n",
        "    # Adan Optimizer. Adaptive Nesterov momentum with gradient difference\n",
        "    path = [start.copy()]\n",
        "    theta = start.copy()\n",
        "    m = np.zeros(2)  # First moment\n",
        "    v = np.zeros(2)  # Gradient difference\n",
        "    n = np.zeros(2)  # Second moment\n",
        "    grad_prev = grad_func(start[0], start[1])\n",
        "\n",
        "    for t in range(1, iterations + 1):\n",
        "        grad = grad_func(theta[0], theta[1])\n",
        "\n",
        "        grad_norm = np.linalg.norm(grad)\n",
        "        if grad_norm > clip_value:\n",
        "            grad = grad * (clip_value / grad_norm)\n",
        "\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "\n",
        "        grad_diff = grad - grad_prev\n",
        "        v = beta2 * v + (1 - beta2) * grad_diff\n",
        "\n",
        "        combined_grad = grad + beta2 * grad_diff\n",
        "        n = beta3 * n + (1 - beta3) * (combined_grad ** 2)\n",
        "\n",
        "        m_hat = m / (1 - beta1 ** t)\n",
        "        v_hat = v / (1 - beta2 ** t)\n",
        "        n_hat = n / (1 - beta3 ** t)\n",
        "\n",
        "        theta = theta - lr * (m_hat + beta2 * v_hat) / (np.sqrt(n_hat) + epsilon)\n",
        "\n",
        "        if np.any(np.isnan(theta)) or np.any(np.isinf(theta)):\n",
        "            print(f\"Warning: Adan diverged at iteration {t}. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        path.append(theta.copy())\n",
        "\n",
        "        grad_prev = grad.copy()\n",
        "\n",
        "    return np.array(path)\n",
        "\n",
        "# Run all optimizers\n",
        "\n",
        "path_gd = gradient_descent(gradient_func, start_point, learning_rate, num_iterations)\n",
        "path_sgd = stochastic_gradient_descent(gradient_func, start_point, learning_rate, num_iterations)\n",
        "path_batch = batch_gradient_descent(gradient_func, start_point, learning_rate, num_iterations, batch_size)\n",
        "path_nesterov = nesterov_gradient_descent(gradient_func, start_point, learning_rate, num_iterations)\n",
        "path_adam = adam_optimizer(gradient_func, start_point, learning_rate, num_iterations)\n",
        "path_adan = adan_optimizer(gradient_func, start_point, learning_rate, num_iterations)\n",
        "\n",
        "# Calculate final values and distances to optimum\n",
        "optimizers = {\n",
        "    'Gradient Descent': path_gd,\n",
        "    'SGD': path_sgd,\n",
        "    'Mini-Batch GD': path_batch,\n",
        "    'Nesterov': path_nesterov,\n",
        "    'Adam': path_adam,\n",
        "    'Adan': path_adan\n",
        "}\n",
        "\n",
        "diverged_optimizers = []\n",
        "\n",
        "for name, path in optimizers.items():\n",
        "    if len(path) == 0:\n",
        "        print(f\"\\n{name}: Failed to start\")\n",
        "        diverged_optimizers.append(name)\n",
        "        continue\n",
        "\n",
        "    final_pos = path[-1]\n",
        "\n",
        "    # Check if optimization diverged\n",
        "    if np.any(np.isnan(final_pos)) or np.any(np.isinf(final_pos)):\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"DIVERGED\")\n",
        "        print(f\"Completed iterations: {len(path) - 1}\")\n",
        "        diverged_optimizers.append(name)\n",
        "    else:\n",
        "        final_value = objective_func(final_pos[0], final_pos[1])\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"Final position: ({final_pos[0]:.6f}, {final_pos[1]:.6f})\")\n",
        "        print(f\"Final function value: {final_value:.6f}\")\n",
        "        print(f\"Total iterations: {len(path) - 1}\")\n"
      ],
      "metadata": {
        "id": "g8slUKa95s5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "# Create grid for contour plot\n",
        "x = np.linspace(x_range[0], x_range[1], 400)\n",
        "y = np.linspace(y_range[0], y_range[1], 400)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = objective_func(X, Y)\n",
        "\n",
        "# Plot 1: All paths together\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "contour = ax1.contour(X, Y, Z, levels=30, alpha=0.6, cmap='viridis')\n",
        "ax1.clabel(contour, inline=True, fontsize=8)\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']\n",
        "for (name, path), color in zip(optimizers.items(), colors):\n",
        "    ax1.plot(path[:, 0], path[:, 1], 'o-', label=name, color=color,\n",
        "             markersize=3, linewidth=1.5, alpha=0.7)\n",
        "    ax1.plot(path[0, 0], path[0, 1], 'k*', markersize=15)  # Start point\n",
        "    ax1.plot(path[-1, 0], path[-1, 1], 'o', color=color, markersize=10)  # End point\n",
        "\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_title(f'All Optimizers on {func_name} Function')\n",
        "ax1.legend(fontsize=8)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Individual plots for each optimizer\n",
        "plot_configs = [\n",
        "    (2, 'Gradient Descent', path_gd, 'red'),\n",
        "    (3, 'Stochastic GD', path_sgd, 'blue'),\n",
        "    (4, 'Mini-Batch GD', path_batch, 'green'),\n",
        "    (5, 'Nesterov', path_nesterov, 'orange'),\n",
        "    (6, 'Adam', path_adam, 'purple')\n",
        "]\n",
        "\n",
        "for idx, name, path, color in plot_configs:\n",
        "    ax = plt.subplot(2, 3, idx)\n",
        "    contour = ax.contour(X, Y, Z, levels=30, alpha=0.4, cmap='viridis')\n",
        "    ax.plot(path[:, 0], path[:, 1], 'o-', color=color, markersize=4, linewidth=2)\n",
        "    ax.plot(path[0, 0], path[0, 1], 'k*', markersize=15, label='Start')\n",
        "    ax.plot(path[-1, 0], path[-1, 1], 'ro', markersize=10, label='End')\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title(name)\n",
        "    ax.legend(fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Convergence curves\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (name, path) in enumerate(optimizers.items()):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Calculate function values along the path\n",
        "    values = [objective_func(p[0], p[1]) for p in path]\n",
        "\n",
        "    ax.plot(values, linewidth=2)\n",
        "    ax.set_xlabel('Iteration')\n",
        "    ax.set_ylabel('Function Value')\n",
        "    ax.set_title(f'{name} - Convergence')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Comparison of convergence speeds\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for (name, path), color in zip(optimizers.items(), colors):\n",
        "    values = [objective_func(p[0], p[1]) for p in path]\n",
        "    plt.plot(values, label=name, color=color, linewidth=2)\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Function Value (log scale)')\n",
        "plt.title('Convergence Comparison - All Optimizers')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "\n",
        "# Plot gradient norms\n",
        "plt.subplot(1, 2, 2)\n",
        "for (name, path), color in zip(optimizers.items(), colors):\n",
        "    grad_norms = []\n",
        "    for p in path:\n",
        "        grad = gradient_func(p[0], p[1])\n",
        "        grad_norms.append(np.linalg.norm(grad))\n",
        "    plt.plot(grad_norms, label=name, color=color, linewidth=2)\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Gradient Norm (log scale)')\n",
        "plt.title('Gradient Magnitude - All Optimizers')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "metrics_data = []\n",
        "for name, path in optimizers.items():\n",
        "    values = [objective_func(p[0], p[1]) for p in path]\n",
        "    grad_norms = [np.linalg.norm(gradient_func(p[0], p[1])) for p in path]\n",
        "\n",
        "    # Find iteration where function value drops below threshold (if applicable)\n",
        "    threshold = 1.0\n",
        "    converged_iter = next((i for i, v in enumerate(values) if v < threshold), len(values))\n",
        "\n",
        "    metrics = {\n",
        "        'Optimizer': name,\n",
        "        'Final Value': values[-1],\n",
        "        'Min Value': min(values),\n",
        "        'Final Grad Norm': grad_norms[-1],\n",
        "        'Iterations to threshold': converged_iter\n",
        "    }\n",
        "    metrics_data.append(metrics)\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Final value: {metrics['Final Value']:.8f}\")\n",
        "    print(f\"Minimum value reached: {metrics['Min Value']:.8f}\")\n",
        "    print(f\"Final gradient norm: {metrics['Final Grad Norm']:.8f}\")\n",
        "    print(f\"Iterations to reach f(x) < {threshold}: {metrics['Iterations to threshold']}\")"
      ],
      "metadata": {
        "id": "z-ya4ZvZ6nXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Observations\n",
        "\n",
        "Based on the results, we can observe:\n",
        "\n",
        "1. **Convergence Speed**:\n",
        "   - Adam and Adan typically converge faster than standard GD\n",
        "   - Nesterov shows improved convergence over momentum-based methods\n",
        "   - SGD has noisy but can escape local minima\n",
        "\n",
        "2. **Stability**:\n",
        "   - GD and Mini-Batch GD show smooth, stable convergence\n",
        "   - SGD shows high variance but explores the space well\n",
        "   - Adam/Adan balance speed and stability\n",
        "\n",
        "3. **Final Accuracy**:\n",
        "   - All methods can reach similar final values with proper tuning\n",
        "   - Adaptive methods (Adam, Adan) require less hyperparameter tuning\n",
        "\n",
        "4. **Practical Considerations**:\n",
        "   - Mini-Batch GD is most common in practice (balance of speed and stability)\n",
        "   - Adam is default choice for many deep learning applications\n",
        "   - Adan shows promise for faster convergence in recent research\n"
      ],
      "metadata": {
        "id": "GMxOesOB7FhZ"
      }
    }
  ]
}
